yum install git -y

sudo amazon-linux-extras install java-openjdk11 -y

sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
yum install jenkins -y

systemctl start jenkins

yum install docker -y

systemctl start dcoker

jenkins ALL=NOPASSWD: ALL

or you can run a command to give permission to run docker by any user as below

chmod 777 /var/run/docker.sock 


-----------------------
Class 5 :

A pipline is a set of jobs which are executed one after another

sytemctl start jenkins

*****

http://18.118.10.14:8080/github-webhook/
http://Jenkins-url/github-webhook/

--------------------------------------------------------------------

Class 6:

1)core build plugins 2) report plugins

pmd - programming mistake detector

sample java project : 
1) https://github.com/Sonal0409/DevOpsCodeDemo.git
2) https://github.com/Sonal0409/JavaWebCalculator (incomple pom file)
3) https://github.com/Sonal0409/calcwebapp-sonar.git

pom file consists of 
1)dependicies 

2)plugins 

2)repositorys - are registries from which maven will download the plug in or dependecies

for graddel - build.xml 
for maven pom.xml

manage jenkins - setup maven ny using jenkins

cd --> /var/lib/jenkins   -> hudson.tasks.Maven.xml

1) package 

maven commands are case sensitive

-----------

pipeline {
    
    tools {
        maven 'mymaven'
    }
    
    //here any means execute the pipline on any of the available servers i.e. current VM for now
    agent any
    
    stages {
        
        stage ('Clone Repo') {
            
            steps {
                git 'https://github.com/vjdgithub/DevOpsCodeDemo.git'
            }
            
        }
        
        stage ('Compile Code') {
            
            steps {
                sh 'mvn compile'
            }
            
        }
        
        stage ('Test Code') {
            
            steps {
                sh 'mvn test'
            }
            
        }
        
        stage ('Package Code') {
            
            steps {
                sh 'mvn package'
            }
            
        }
    }
    
}

-------------------------------------------------------------

Class 7:

for review 

stage ('Review Code') {
            
            steps {
                sh 'mvn pmd:pmd'
            }
            
        }

gives pmd.xml  -  all the development mistakes will be here

for coverage - based on the test cases how much code is covered for testeing - jacoco plug in

repository to support cove coverage

https://github.com/vjdgithub/calcwebapp-sonar.git

code coverage is a process in which we generate a report using code coverage tool (ie. jacoco or cobertura etc..) against compiled classes and test cases which helps us to determine which part of java programme has been tetsted and which part lacks testing

code coverage report will be generatd 

unit testing - developer

and testing team - selenium, Cypress and jasmine testing tools for testers   
integration testing - 

performnce testing Jmeter, load runner to write the test cases

all of the above integrated with jenkins

link for .net project - https://github.com/vjdgithub/MSbuildProject.git

https://github.com/vjdgithub/DevOps_ClassNotes/blob/master/JENKINS/MSBuildJenkins.docx
 
parameters are variable which can store values, Dev , Prod - we can choose ad based on parameter i can execute the stage 

boolean or choice parameter

parameters are declared in the parameters section and used in stages for conditions

concept of backing up the jenkins server - s3 bucket

triggers in a pipeline

https://github.com/vjdgithub/DevOps_ClassNotes/blob/master/JENKINS/Triggers_PIPELINE

pipeline {
    
    tools {
        
        maven 'mymaven'
    }
    
    agent any
    
    //parameter that stores set of values --> parameter type = choice
    // default value will be given for choice
    parameters {
        
        choice(name: "ENV", choices: ["", "Dev", "QA"])
    }
    
    stages {
        
        stage("Build in Dev Env"){
            
            when { expression {params.ENV == "Dev"}}
            steps{
                
               git 'https://github.com/vjdgithub/DevOpsCodeDemo.git'
               sh 'mvn pmd:pmd'
               sh 'mvn package'
            }
            
        }
        
        stage("Build in QA Env"){
            
            when { expression {params.ENV == "QA"}}
            steps{
                
               git 'https://github.com/vjdgithub/DevOpsCodeDemo.git'
               sh 'mvn test'
               sh 'mvn package'
            }
            
        }
    }
    
}

Slave Set up :
---------------
yum install git -y
sudo amazon-linux-extras install java-openjdk11 -y
go to tmp folder create jenkins directory
cd /tmp
mkdir jenkinsdir
give jenkinsdir with read and write permissions
chmod -R 777 /tmp/jenkinsdir

change the name of the machine to JenkinsAgent

hostname JenkinsAgent
sudo su -

Agent should be enabled to Random in Manage Jenkins/Security

configure the agents to the master
manual setup via nodes or on the cloud automatically
no use manual setup in using nodes in configuration 

labels are used for selecting the desired VM on the linux

use it to build the jobs on maching lable expressions

host address is private ip as both the instances are in the same network


--jfrog artifactory conncet using ssh and deploy artifacts

all the jobs run on any node labled 'linux_node'

pipeline {
    
    tools {
        maven 'mymaven'
    }
    
    //here any means execute the pipline on any of the available servers i.e. current VM for now
    agent { label 'linux_node'  }
    
    stages {
        
        stage ('Clone Repo') {
            
            steps {
                git 'https://github.com/vjdgithub/DevOpsCodeDemo.git'
            }
            
        }
        
        stage ('Compile Code') {
            
            steps {
                sh 'mvn compile'
            }
            
        }
        
        stage ('Review Code') {
            
            steps {
                sh 'mvn pmd:pmd'
            }
            
        }
        
        stage ('Test Code') {
            
            steps {
                sh 'mvn test'
            }
            
        }
        
        stage ('Package Code') {
            
            steps {
                sh 'mvn package'
            }
            
        }
    }
    
}

One job on Master server and one job on the any node labled 'linux_node'

pipeline {
    
    tools {
        
        maven 'mymaven'
    }
    
    agent none
    
    //parameter that stores set of values --> parameter type = choice
    // default value will be given for choice
    parameters {
        
        choice(name: "ENV", choices: ["", "Dev", "QA"])
    }
    
    stages {
        
        stage("Build in Dev Env"){
            
            agent { label 'linux_node'  }
            
            when { expression {params.ENV == "Dev"}}
            steps{
                
               git 'https://github.com/vjdgithub/DevOpsCodeDemo.git'
               sh 'mvn pmd:pmd'
               sh 'mvn package'
            }
            
        }
        
        stage("Build in QA Env"){
            
            agent any
            
            when { expression {params.ENV == "QA"}}
            steps{
                
               git 'https://github.com/vjdgithub/DevOpsCodeDemo.git'
               sh 'mvn test'
               sh 'mvn package'
            }
            
        }
    }
    
}

//how to set up windos slave

https://github.com/vjdgithub/DevOps_ClassNotes/blob/master/JENKINS/MVNJENKINS_NOTES.txt

----------
Class 8 :

Dockerhub - it is a online open source repository where Docker places its images which is binary file

yum install docker -y

systectl start docker

docker info

private registries are maintained by docker administrator

Registry - a location to maintain binary file
Repository - a location to maintain source code

//to check the images
docker images

docker pull ubantu

//to check the all the container
docker ps -a 

//to check the running container
docker ps 

docker login -u <your-username>

docker login -u vjddocker 

You may need to use a specific registry version, for me this worked.
docker login registry-1.docker.io/v1

Ansible docker is primarly only for linux

for pulling no need of username and pwd but for pushing it requires

tags are different versions of the images

an image is built by using editable docker file when ever you build a new image new version of the image is introduced that is called a s tag. 

//to pull specific version
docker pull imagename:tagname

//for pulling puclic images you do not need credentials
docker pull sonal04/base01 

 docker run ubuntu

when we run image again every time new container will come

docker run is a dual command - first check if image is avaiable locally or not otherwise get it will pull and create the container
//ceate a container with name
docker run --name <containerName> imageName
docker run --name cont1 ubuntu

//for user to be attached to the container in foreground mode use -it
docker run --name cont2 -it ubuntu

container id and name are unique

//to know the os 
cat /etc/os-release

--you can not run docker commands on the container -- docker wont install in container

--come out of the container and keep the container running
CTL pq

---to attach to the container again - you can attach only to a running container
docker attach containername

--when we exit we come out of the container and container exited
-- we can start containers again only which are created in forground mode
docker start containername
-- to stop the container
docker stop containername

-- you can not run windows container on linux and vice versa

- for every service it is a micro service i.e. your application is running independent of the others
- for ecommerce application you will create may services i.e. containers

---
to delete the container
docker rm -f <containername?
---to delete all  the containers
docker rm -f $(docker ps -aq)

------------
class 9 :
-----------
Access the container in detached mode (-d)

dangling images - images in the machine without a container attached

to remove the dangling images command is 
docker system prune
docker system prune --all

docker system prune --images [to remove only images]

to inspect if there are any containers attached the command is

docker inspect <imagename>

to create nginx image with a name in a detached mode

docket run --name web -d nginx

you can acces the detached container i.e. web containers from the host machine and out side of host machine also i.e via internet

to access the container from the host machine and execute a command as   

docker exce  -it<for attachment> <containername> <command>

docker exec -it web /bin/bash

by default nginx container comes with application

 cd usr/share/nginx
 ls
 cd html
 there is default index page
when i exit i am out of the process and container is still running

to acceess that applicaion we need use concept of port mapping/port forwarding

every we container will have a port number attached with it

** port mapping needs to done at the docker run command itself

---in docker once the container is created we can not do port mapping but in kubernatics we can do 

docker run --name web2 -d -p 9090:80 nginx

or let docker decide the port number

docker run --name web3 -d -P(CAPITAL P) nginx
docker run --name web3 -d -P nginx

---when even when docker is installed by default docker creates its own network as Docker zero network (type bridge) it has a default sub net range and the container get ip addresses of that sub net range

--we can not access the container via ip but can ccess by mapped port number

-- docker that container is created in production environment via archestration tool will have port number in the range of 30000 to 32767

to know the network

docker network ls 

-------

customize the container and convert in to a image

come out of the container CTL + p



docker commit basecont myimage01

docker login 

docker push image01 - access denied as you are pushing to docker hub library

hence change the image name as 

docker tag <OldImageName> <DockerAccountId/NewImageName> 

docker tag myimage01 vjddocker/myiamge01
docker push vjddocker/myiamge01

recent interview questions :

what are dangling images 
how to push docker into a aws private registry - vedio recording is there
difference between docker stocp and kill command 

-----
Class 10
-----

before writing a docker file plan first waht you need in container

by default Run commands run in the root directory otherwise you can choose

What is the difference between Copy and Add keyworks

docker build . (. means current directory-> build command allways searches for dockerfile) other specify the path as below
docker build -p /path/file

if  we want to tag the image as below

docker build -t myimage01:tag .

--image is a collection of binary files . these set of images together consistitues final image
-- docker at commits images one by one -- commit is what  uses in the background and what ever activities what docker is using to build your final image is docker cache

docker build -t myimage01:tag . 

in one directory only one docker file should be there
you need not give the name of the dockerfile as the file name is always dockerfile
by default docker docker doesnot validate the dockerfile . errors come runtime. however there is one dockerfile linter and you can validate the dockerfile

docker history <imagename>/<imageid>

when we delete an image iamge layers shown in the docker history will be be deleted but not docker cache. if you wanrt to delete the docker cache you need to use below caommand

docker system prune --all

Difference between CMD and ENTRYPOINT Keywords

both will execute after the instance is launched

between CMD and ENTRYPOINT --> ENTRYPOINT gets precedence - it is the final command

In the dockerfile if the developer mentions only CMD that means the the docker developer is giving an oppertunity to the another deveoper who is running the image to pass a new command at run time. so when i run the busybox image if i dont want default to be "sh" and if i want some other command to be given at runtime after the container is launched a below

actual busybox dockerfile is 

FROM scratch
ADD busybox.tar.xz /
CMD ["sh"]

docker run -d busybox sleep 6000

above "sh" command is replaced with "sleep 6000" at run time

if it is placed in at entry point then that command can not be replaced by entrypoint command. but when we pass new command at run time when docker file using entrypoint, the newly given command at run time is just gets appended to the entry point command.

docker run -d nginx slep 6000 

how to create multi stage Build dockerfile - home work

------
Class 11
------

Deploy a node js application using image node from

https://github.com/Sonal0409/nodejsappDockerfile.git

we can not do port mapping in the dockerfile . while creating container only we need to port mapping

docker system prune --all
docker rm -f $(docker ps -aq)

clone the above repo and go inside the repo, remove all the existing images and processes and then do the following

docker build -t mynodeapp .

docker run -d -P mynodeapp

docker ps -a

press enter to execute the command
press CTL + C - to dont run and come out

on our server does not have root permissions to run docker command

got to /etc/sudoers file, inside this file go to root section and give jenkins user nopassword permission as below

jenkins ALL=NOPASSWD: ALL

or you can run a command to give permission to run docker by any user as below

chmod 777 /var/run/docker.sock 

pipeline {
    
    tools {maven 'mymaven'}
    
    agent any 
    
    stages {
        
        stage ('Clone Repo') {
            
            steps {
                git 'https://github.com/vjdgithub/DevOpsCodeDemo.git'
            }
            
        }
        
        stage ('Compile Code') {
            
            steps {
                sh 'mvn compile'
            }
            
        }
        
        stage ('Review Code') {
            
            steps {
                sh 'mvn pmd:pmd'
            }
            
        }
        
        stage ('Test Code') {
            
            steps {
                sh 'mvn test'
            }
            
        }
        
        stage ('Package Code') {
            
            steps {
                sh 'mvn clean install package'
            }
            
        }
        
        stage ('Copy the War file from target directory to current diroctory') {
            
            steps {
                sh 'cp /var/lib/jenkins/workspace/CICDpipeline/target/addressbook.war .'
            }
            
        }
        stage ('Build Image') {
            
            steps {
                sh 'docker build -t myappjenkins .'
            }
            
        }
        stage ('Deploy the Image') {
            
            steps {
                sh 'docker run -d -P myappjenkins'
            }
            
        }
        
    }
}

for java application deployed in tomcat access the appliacation appended with war file name as below

http://18.223.171.192:32769/addressbook

pipeline {
    
    tools {maven 'mymaven'}
    
    agent any 
    
    stages {
        
        stage ('Clone Repo') {
            
            steps {
                git 'https://github.com/vjdgithub/DevOpsCodeDemo.git'
            }
            
        }
       
                stage ('Build the Code') {
            
            steps {
                sh 'mvn clean install package'
            }
            
        }
        
        stage ('Copy the War file from target directory to current diroctory') {
            
            steps {
                sh 'cp /var/lib/jenkins/workspace/CICDpipeline2/target/addressbook.war .'
            }
            
        }
        stage ('Build Image') {
            
            steps {
                sh 'docker build -t myappjenkins2 .'
            }
            
        }
        stage ('Deploy the Image') {
            
            steps {
                sh 'docker run -d -P myappjenkins2'
                sh 'docker ps -a'
            }
            
        }
        
    }
}

------
every time bulds, if we want to build different image then use jenkins variable

jenkins variable BUILD_NUMBER

stage ('Build Image') {
            
            steps {
                sh 'docker build -t myappjenkins2:$BUILD_NUMBER .'
            }
            
        }
        stage ('Deploy the Image') {
            
            steps {
                sh 'docker run -d -P myappjenkins2:$BUILD_NUMBER'
                sh 'docker ps -a'
            }
            
        }

in docker we have to manually delete the older conatiners

we have to use any of the archestartion tools to automatically delete the older containers like docker swarm or kubernatics

-->  Multi Branch Deployemnet - Multi branch Pipeline

git clone https://github.com/vjdgithub/DevOpsCodeDemo.git

to remove old origin value

git remote rm origin

to add new 

git remote add origin https://github.com/vjdgithub/MultiBranchDeployment.git

to rename the master branch 

git branch -M master addressbook

to create branch on a remote repository with a branch name, source code and jenkins file

git push origin addressbook

similary for another app

cd nodejsappDockerfile

git remote rm origin

to add new 

git remote add origin https://github.com/vjdgithub/MultiBranchDeployment.git

to rename the master branch 

git branch -M master nodejs

to create branch on a remote repository with a branch name, source code and jenkins file

git push origin nodejs 

and write dockersfile and jenkins file in the github

similar multi branch scenario

https://github.com/Sonal0409/JavaWebCalculator

---------------------------------
Class 13:
---------------------------------

All the Orchestration tools works under custer mode

-- Cluster is a set of virtual machines/Nodes with a container tool (CRI) installed on it . i.e. docker

-- Roncher is a tool that manages cluster

-- Orchestration tool allways depend on conatiner runtime (CRI)

-- For Docker swarm CRI - Docker
-- For Kubernatics it can be Docker or Container D
-- EKS uses docker in the back ground

-- A set of containers is a Micro Service 

-- We will create a relationship between these VMS/ Nodes and one of them is leader and others are worker nodes.
-- On Master you intialise Docker swarm on it, then it will become master. It is als called as leader node as swarm initiated on it.
-- On one cluster only one leader 
-- when initiate swarm, there will be a token generated address and port number of the leader. this token will be copied to your worker nodes in your cluster
--In docker sworm there is raft algirithm, which takes the request of orchestartion command

-- By default swarm is inactive. we need to activate in only on one of the machine so that it can become manager

-- docker swarm init
--- copy the key command on the other VMs/nodes on the cluster and execute so that they become worker nodes of this swarm network

-- to know the nodes status belo command

docker node ls

--- you can promote worker to become manager and demote vice versa but atleast a given time there should be one leader otherwise your swarm cluster become headless

--- if we execute below command on any of the worker nodes it will become manager but not as leader . but its status is reachable to leader in case of leader is leaving

docker swarm join-token manager

-- docker swarm join-token worker - the node will join as worker

-- to selct the new leader via the polling

-- docker sward does provde one object that docker does not provide i.e. service object ( High level object)

docker service create --name mysvc --replicas 4 -p 8081:80 nginx

docker service ls

docker service ps mysvc

-- in case of docker swarm containers are scheduled on the manager node
--in case of kubernatics containers are scheduled on the worker node

watch docker service ls - every 2 sec this command is going to run

docker rm -f <contanier id >  - to delete container

docker service ps mysvc | grep Running

docker service scale mysvc=6 -to scale up/down

-- in docker swarm scaling up and down shold be done manually

docker service scale mysvc=0

docker service rm mysvc

network load balancing
docker service create --name mysvc --replicas 2 -p 8282:3000 sonal04/samplepyapp:v1

while true; do curl http://18.191.145.175:8282/; sleep 1; echo " "; done

give CTL + C to stop

docker service scale mysvc=4

docker service update --image sonal04/samplepyapp:v2 mysvc

it is of round rabbin fashion raft algirithm will take care of it

docker service rm helloworld

--------------------------------------------------------------
Class 13
---------
docker - bridge network
docker swarm - overlay network

incase of container we have to install CNI

for K8S - Dedicated CNI - Weave network, calico network, etc...
every cloud provider will give their own network for K8S Cluster


Difference between CNI (to get Ip addresses and ports in cluster network ) and 
Cube Proxy is a networking rule to access your container

docker network ls

ip a

docker inspect <network id>

to create a new network 

docker network create --driver bridge net1

docker run -d -P --network net1 nginx

you can also change sub net and ip range by using below help

docker network create --help

docker system prune --all

kubectl get nodes

kubectl run pod1 --image nginx 

kubectl describe pod pod1 | less

kubectl get pods -n kube-system

kubectl logs 

kubectl get nodes -o wide

gcloud container clusters get-credentials CLUSTER_NAME --region REGION --project PROJECT_ID

service account is a to token to create the pod

Push approach - Ansible

and pull approach - K8s
------------------------------------------------------------
Class 14
--------
A set of maps is a object

mkdir mykubefiles

cd mykubefiles

--------

vim pod-definition.yml

apiVersion: v1
kind: Pod
metadata:
   name: podnginx
   labels:
      author: jagadeesh
      app: nginx
      type: webserver
spec:
  containers:
    - name: c1
      image: nginx

-------------

kubectl get pods
kubectl delete pod pod1

kubectl create -f pod-definition.yml

kubectl get pods
kubectl get pods -o wide
kubectl describe pod podnginx
kubectl get pods --show-labels
kubectl get pods -l author=jagadeesh

labels are used to query the pods and selecting the pods

vim test-pod.yml

https://github.com/Sonal0409/Vodafone-Kubernetes-Notes/blob/main/Day2-Notes/Service/ClusterIPDemo/UbuntuPod.yml
apiVersion: v1

kind: Pod

metadata:

 name: testpod

 labels:

  role: test

spec:

 containers:

  - name: c1

    image: ubuntu

    args: [/bin/bash, -c, 'sleep 6000']



to get ip address of the terminal 

kubectl get pods -o wide

10.244.1.5

to ping one pod with another pod with ip 

kubectl exec -it testpod -- /bin/bash

apt-get update

apt-get install curl -y

curl 10.244.1.5

curl podnginx

https://github.com/Sonal0409/Vodafone-Kubernetes-Notes/blob/main/Day2-Notes/Service/ClusterIPDemo/Service-clusterIP.yml

vim service.yml
---
apiVersion: v1
kind: Service
metadata: 
 name: mysvc1
spec:
 type: ClusterIP
 selector:
  type: webserver
 ports:
 - targetPort: 80
   port: 80

kubectl create -f service.yml


kubectl get service

in testpod 
curl <ip address of service> 
curl <service name /domain name>

curl mysvc1
curl mysvc1:80

kubectl get endpoints mysvc1
-----------------------------------------------------------
Class 15:
---------

to access the container outside the cluster we need to do port mapping (Nodeport service)

watch kubectl get pods --show-labels 


vim pod-definition.yml

apiVersion: v1
kind: Pod
metadata:
   name: pod1
   labels:
      type: webserver
spec:
  containers:
    - image: nginx
      name: c1
      
kubectl create -f pod-definition.yml

vim service.yml
---
apiVersion: v1
kind: Service
metadata: 
 name: mysvc1
spec:
 type: ClusterIP
 selector:
  type: webserver
 ports:
 - targetPort: 80
   port: 80

kubectl get svc

vim service2.yml
---
apiVersion: v1
kind: Service
metadata: 
 name: mysvc-nodeport
spec:
 type: NodePort
 ports:
 - targetPort: 80
   port: 80
 selector:
  type: webserver

kubectl get svc

kubectl get nodes -o wide

<ip address of Work node>:31836

vim service3.yml
---
apiVersion: v1
kind: Service
metadata: 
 name: mysvc-lb
spec:
 type: LoadBalancer
 ports:
 - targetPort: 80
   port: 80
 selector:
  type: webserver

now we get external ip . only id address in the browser is sufficient

kubectl delete svc  mysvc-lb

when we delete load balancer service - extrernal load balance which created on GCP will also be deleted

kubectl delete svc  mysvc-nodeport

kubectl create -f pod-definition.yml
--------------------
scaling up and scaling down using replica set

kubectl delete pod pod1

scaling up and scaling down 

vim deployment.yml


apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
     name: nginxpod
     labels:      
      app: nginx
    spec:
     containers:
     - name: c1
       image: leaddevops/kubeserve:v1

kubectl get pods

kubectl create -f deployment.yml

kubectl get all

watch kubectl get replicaset

kubectl delete pod nginx-7d4bc45fb9-6nk9s

kubectl scale deployment nginx --replicas 5






------------------------------------------------------------
Class 17
---------

docker system prune --all

hostname AnsibleController

sudo amazon-linux-extras install epel

yum install ansible

ansible --version

cd /etc/ansible to get host file

adduser ansiuser
passwd ansiuser

vim /etc/ssh/sshd_config

vim /etc/sudoers

systemctl restart sshd

su - ansiuser
ssh-keygen and press enter and enter

generated the public key /home/ansiuser/.ssh/id_rsa.pub

use the command to copy public key to worker nodes

ssh-copy-id -i ansiuser@<worker nodes private id as both the nodes are in the same network>
ssh-copy-id -i ansiuser@172.31.44.94

create inventory in cd etc/ansible

sudo vim hosts

ansible webserver -m ping

ansible testserver -m ping














